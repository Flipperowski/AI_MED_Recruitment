{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caabd541",
   "metadata": {},
   "source": [
    "# Cardiomegaly Classification – Model Comparison\n",
    "\n",
    "This notebook presents a machine learning solution for the classification of hypertrophic cardiomyopathy (cardiomegaly) based on geometric and imaging features.  \n",
    "The main objective is to distinguish between a healthy heart and a diseased heart using classical machine learning algorithms.  \n",
    "\n",
    "The dataset is provided in a CSV file (`task_data.csv`), containing pre-selected features.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72e5bd-78a8-48fd-acf8-a69a58b223ad",
   "metadata": {},
   "source": [
    "## Libraries and Tools\n",
    "\n",
    "We use the following technologies:\n",
    "- **Python 3.11+** – programming language.\n",
    "- **NumPy** – numerical computations, array handling.\n",
    "- **Pandas** – dataset loading, preprocessing, tabular operations.\n",
    "- **Scikit-learn** – machine learning models, preprocessing, cross-validation, metrics.\n",
    "- **Matplotlib & Seaborn** – data visualization, plots and heatmaps.\n",
    "- **Jupyter Notebook** – interactive environment for development and experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c531282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, RepeatedStratifiedKFold, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc, f1_score, roc_auc_score, precision_recall_curve, average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172a09de",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration\n",
    "\n",
    "In this step:\n",
    "1. We load the dataset using `pandas.read_csv()`.\n",
    "2. Perform a quick check of the dataset shape, column names, and missing values.\n",
    "3. Split features (`X`) and target (`y`).\n",
    "\n",
    "This ensures that the dataset is correctly structured before preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6868af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the data from CSV (80% training, 20% testing)\n",
    "data = pd.read_csv(\"task_data.csv\")\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "numeric_cols = [\n",
    "    \"Heart width\", \"Lung width\", \"CTR - Cardiothoracic Ratio\", \"xx\", \"yy\", \"xy\", \"normalized_diff\",\n",
    "    \"Inscribed circle radius\", \"Polygon Area Ratio\", \"Heart perimeter\", \"Heart area\", \"Lung area\"\n",
    "]\n",
    "\n",
    "#Repairing data types\n",
    "for col in numeric_cols:\n",
    "    data[col] = data[col].astype(str).str.replace(\",\", \".\", regex=True).astype(float)\n",
    "    \n",
    "X = data[[\n",
    "    \"Heart width\", \"Lung width\", \"CTR - Cardiothoracic Ratio\",\n",
    "    \"xx\", \"yy\", \"xy\", \"normalized_diff\",\n",
    "    \"Inscribed circle radius\", \"Polygon Area Ratio\",\n",
    "    \"Heart perimeter\", \"Heart area\", \"Lung area\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4b563a-f136-4fe6-bbbc-07429073a2fe",
   "metadata": {},
   "source": [
    "## Train-Test Split and Preprocessing\n",
    "\n",
    "- The dataset is split into training (80%) and testing (20%) subsets using `train_test_split`.\n",
    "- Features are standardized using `StandardScaler` to ensure that all input features are on a comparable scale.\n",
    "- Standardization is especially important for algorithms that are sensitive to feature magnitudes (e.g., SVM, Logistic Regression, KNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a12eec-6c17-48fa-879e-bbd5b5ea97a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting targeted column (Cardiomegaly)\n",
    "y = data[\"Cardiomegaly\"]\n",
    "\n",
    "#Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Creating a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#Fitting and applying scaler\n",
    "X_scaled_train = scaler.fit_transform(X_train)\n",
    "X_scaled_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb6b797",
   "metadata": {},
   "source": [
    "**k-Nearest Neighbors (KNN)** (`KNeighborsClassifier`)\n",
    "   - Instance-based learning method.\n",
    "   - Classifies a sample based on the majority class of its k nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befdd14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining hyperparameters\n",
    "param_grid = {\n",
    "    \"model__n_neighbors\": [1, 3, 5, 7, 9, 11, 13, 15, 17, 19],\n",
    "    \"model__weights\": [\"uniform\", \"distance\"],\n",
    "    \"model__metric\": [\"minkowski\", \"manhattan\", \"euclidean\"],\n",
    "    \"model__p\": [1, 2]\n",
    "}\n",
    "\n",
    "#Setting up the cross-validation strategy\n",
    "rskf = RepeatedStratifiedKFold(\n",
    "    n_splits=5,\n",
    "    n_repeats=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#Applying K-Nearest Neighbors (KNN) Classifier\n",
    "pipe_knn = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "#Initializing the Grid Search for the KNN model\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipe_knn,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=rskf,\n",
    "    verbose=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#Training\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_knn = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1dcad3",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "- Decision tree is good for both classification (categorizing data) and regression (predicting continuous values) tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9d08c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "clf_tree = DecisionTreeClassifier(\n",
    "    max_depth=5,\n",
    "    criterion='entropy',\n",
    "    min_samples_split=8,\n",
    "    min_samples_leaf=8,\n",
    "    class_weight=None,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#Training\n",
    "clf_tree.fit(X_train, y_train)\n",
    "\n",
    "cv_score = np.round(cross_val_score(clf_tree, X_train, y_train),2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a47273-ab59-4034-befd-b0cffc6e9697",
   "metadata": {},
   "source": [
    "**Support Vector Machine (SVM)** (`SVC`)\n",
    "   - Finds an optimal hyperplane to separate classes.\n",
    "   - Works well with standardized data and non-linear kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f4b8a9-50f9-441b-88eb-62125e604845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support Vector Machine (SVM)\n",
    "param_grid_svm = {\n",
    "    \"model__C\": [0.1, 1, 3, 10],\n",
    "    \"model__gamma\": [\"scale\", \"auto\", 0.01, 0.1, 1],\n",
    "    \"model__kernel\": [\"rbf\", \"poly\", \"sigmoid\"]\n",
    "}\n",
    "\n",
    "#Applying SVM Classifier\n",
    "pipe_svc = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", SVC(probability=True))\n",
    "])\n",
    "\n",
    "#Initializing the Grid Search for the SVC model\n",
    "grid_search_svm = GridSearchCV(\n",
    "    estimator=pipe_svc,\n",
    "    param_grid=param_grid_svm,\n",
    "    cv=5,\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#Training\n",
    "grid_search_svm.fit(X_train, y_train)\n",
    "\n",
    "best_svm = grid_search_svm.best_estimator_\n",
    "\n",
    "cv_score = np.round(cross_val_score(best_svm, X_train, y_train), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a64b6e-1bd2-40ad-9372-7c95414732fd",
   "metadata": {},
   "source": [
    "**Logistic Regression** (`LogisticRegression`)\n",
    "   - A linear model for binary classification.\n",
    "   - Provides interpretable coefficients and well-calibrated probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0984a024-5d16-42b7-ba41-859eb7fc9a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "#Setting up GridSearchCV\n",
    "param_grid_lr = [\n",
    "    {\n",
    "        \"model__penalty\": [\"l1\"],\n",
    "        \"model__C\": [0.01, 0.1, 1, 3, 10],\n",
    "        \"model__solver\": [\"liblinear\", \"saga\"]\n",
    "    },\n",
    "    {\n",
    "        \"model__penalty\": [\"l2\"],\n",
    "        \"model__C\": [0.01, 0.1, 1, 3, 10],\n",
    "        \"model__solver\": [\"liblinear\", \"saga\", \"lbfgs\", \"newton-cg\"]\n",
    "    },\n",
    "    {\n",
    "        \"model__penalty\": [\"elasticnet\"],\n",
    "        \"model__C\": [0.01, 0.1, 1, 3, 10],\n",
    "        \"model__solver\": [\"saga\"],\n",
    "        \"model__l1_ratio\": [0.3, 0.5, 0.7]\n",
    "    }\n",
    "]\n",
    "\n",
    "#Applying Logistic Regression Scaler\n",
    "pipe_lr = Pipeline(steps=[\n",
    "    (\"scaker\", StandardScaler()),\n",
    "    (\"model\", LogisticRegression(max_iter=50000))\n",
    "])\n",
    "\n",
    "#Initializing the Grid Search for the LR\n",
    "grid_search_lr = GridSearchCV(\n",
    "    estimator=pipe_lr,\n",
    "    param_grid=param_grid_lr,\n",
    "    cv=5,\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Training\n",
    "grid_search_lr.fit(X_train, y_train)\n",
    "best_lr = grid_search_lr.best_estimator_\n",
    "\n",
    "cv_score = np.round(cross_val_score(best_lr, X_train, y_train, cv=5, scoring=\"accuracy\"), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4d1123-c95d-4b14-be3c-c7c9e3db1f3c",
   "metadata": {},
   "source": [
    "**Random Forest Classifier** (`RandomForestClassifier`)\n",
    "   - An ensemble of decision trees.\n",
    "   - Reduces overfitting and captures complex interactions between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc9e2bf-f509-44e3-9bcc-d89fc4dd4cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Classifier\n",
    "clf_rf = RandomForestClassifier(\n",
    "    max_depth=6,\n",
    "    min_samples_split=6,\n",
    "    n_estimators=125,\n",
    "    min_samples_leaf=2,\n",
    "    max_features='sqrt'\n",
    ")\n",
    "\n",
    "#Training\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "cv_score = np.round(cross_val_score(clf_rf, X_train, y_train), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daf0bf0-d9d4-4dab-9810-b46e862f452c",
   "metadata": {},
   "source": [
    "## Cross-Validation and Model Evaluation\n",
    "\n",
    "- Each model is trained and evaluated using 5-fold cross-validation (`cross_val_score`).\n",
    "- Evaluation metrics include:\n",
    "  - **Accuracy** – overall correctness.\n",
    "  - **Precision** – proportion of correctly identified positives among predicted positives.\n",
    "  - **Recall** – proportion of correctly identified positives among actual positives.\n",
    "  - **F1-Score** – harmonic mean of precision and recall.\n",
    "  - **ROC AUC** – ability to distinguish between classes across thresholds.\n",
    "\n",
    "This step ensures a robust and fair comparison of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8c06c2-2033-4f2f-9af3-ef68aca57b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation function\n",
    "#Evaluate one model\n",
    "def evaluate_single_model(model, X, y, cv=5):\n",
    "    print(f\"\\nEvaluating model: {model.__class__.__name__}\")\n",
    "    \n",
    "    # Accuracy\n",
    "    acc = cross_val_score(model, X, y, cv=cv, scoring=\"accuracy\")\n",
    "    print(\"\\nAccuracy scores:\", np.round(acc, 2))\n",
    "    print(f\"Mean Accuracy: {np.mean(acc):.3f} | Std: {np.std(acc):.3f}\")\n",
    "\n",
    "    # F1\n",
    "    f1 = cross_val_score(model, X, y, cv=cv, scoring=\"f1\")\n",
    "    print(\"\\nF1 scores:\", np.round(f1, 2))\n",
    "    print(f\"Mean F1: {np.mean(f1):.3f} | Std: {np.std(f1):.3f}\")\n",
    "\n",
    "    # ROC AUC\n",
    "    auc = cross_val_score(model, X, y, cv=cv, scoring=\"roc_auc\")\n",
    "    print(\"\\nROC AUC scores:\", np.round(auc, 2))\n",
    "    print(f\"Mean ROC AUC: {np.mean(auc):.3f} | Std: {np.std(auc):.3f}\")\n",
    "\n",
    "\n",
    "#Evaluate all models at once\n",
    "def evaluate_all_models(models, X, y, cv=5):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n\\n--- Evaluating {name} ---\")\n",
    "        evaluate_single_model(model, X, y, cv=cv)\n",
    "\n",
    "        scores = cross_validate(\n",
    "            model, X, y, cv=cv, scoring=[\"accuracy\", \"f1\", \"roc_auc\"]\n",
    "        )\n",
    "        results[name] = {\n",
    "            \"acc_mean\": scores[\"test_accuracy\"].mean(),\n",
    "            \"f1_mean\": scores[\"test_f1\"].mean(),\n",
    "            \"roc_mean\": scores[\"test_roc_auc\"].mean(),\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222eddf9-a6e4-4061-a370-12ab45242ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Set Evaluation\n",
    "results = evaluate_all_models(models, X_train, y_train)\n",
    "\n",
    "results = {}\n",
    "\n",
    "models = {\n",
    "    \"KNN\": grid_search_knn.best_estimator_,\n",
    "    \"Decision Tree\": clf_tree,\n",
    "    \"SVM\": grid_search_svm.best_estimator_,\n",
    "    \"Logistic Regression\": grid_search_lr.best_estimator_,\n",
    "    \"Random Forest\": clf_rf\n",
    "}\n",
    "\n",
    "#Cross validation\n",
    "for name, model in models.items():\n",
    "    scores = cross_validate(\n",
    "        model, X_train, y_train,\n",
    "        cv=5,\n",
    "        scoring=[\"accuracy\", \"f1\", \"roc_auc\"]\n",
    "    )\n",
    "    results[name] = {\n",
    "        \"acc_mean\": scores[\"test_accuracy\"].mean(),\n",
    "        \"f1_mean\": scores[\"test_f1\"].mean(),\n",
    "        \"roc_mean\": scores[\"test_roc_auc\"].mean(),\n",
    "        \"acc_std\": scores[\"test_accuracy\"].std(),\n",
    "        \"f1_std\": scores[\"test_f1\"].std(),\n",
    "        \"roc_std\": scores[\"test_roc_auc\"].std()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bcaf2c-4b99-4477-bd11-ef4ad2fdfe11",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We visualize model performance with:\n",
    "- **Confusion Matrix** – shows correctly and incorrectly classified samples.\n",
    "- **ROC Curve** – plots True Positive Rate vs. False Positive Rate, with AUC score.\n",
    "- **Precision-Recall Curve** – shows trade-off between precision and recall, with Average Precision (AP).\n",
    "\n",
    "These visualizations provide insight into model strengths and weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9eabfb-3178-41d7-9f57-6834edd433a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_names = list(results.keys())\n",
    "acc = [results[m][\"acc_mean\"] for m in models_names]\n",
    "f1  = [results[m][\"f1_mean\"] for m in models_names]\n",
    "roc = [results[m][\"roc_mean\"] for m in models_names]\n",
    "\n",
    "x = np.arange(len(models_names))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.bar(x - width, acc, width, label=\"Accuracy (CV mean)\")\n",
    "ax.bar(x, f1, width, label=\"F1 (CV mean)\")\n",
    "ax.bar(x + width, roc, width, label=\"ROC AUC (CV mean)\")\n",
    "\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Model Comparison - Accuracy, F1, ROC AUC\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models_names)\n",
    "ax.legend()\n",
    "plt.ylim(0, 1.1)\n",
    "plt.show()\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n\\n=== {name} ===\")\n",
    "    visualize_model_performance(model, X_test, y_test, model_name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0b97b3-d477-4ec1-a132-f7a90b7a6cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_performance(model, X_test, y_test, model_name=\"Model\"):\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # ROC/PR\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_score = model.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        y_score = model.decision_function(X_test)\n",
    "    else:\n",
    "        print(f\"{model_name} nie obsługuje ROC/PR (brak predict_proba/decision_function).\")\n",
    "        y_score = None\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Oranges\", cbar=False)\n",
    "    plt.title(f\"{model_name} - Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "    if y_score is not None:\n",
    "        # ROC Curve\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "        roc_auc = roc_auc_score(y_test, y_score)\n",
    "        plt.figure(figsize=(6,5))\n",
    "        plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"AUC = {roc_auc:.3f}\")\n",
    "        plt.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
    "        plt.title(f\"ROC Curve - {model_name}\")\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "        # Precision-Recall Curve\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "        ap = average_precision_score(y_test, y_score)\n",
    "        plt.figure(figsize=(6,5))\n",
    "        plt.plot(recall, precision, color=\"blue\", lw=2, label=f\"AP = {ap:.3f}\")\n",
    "        plt.title(f\"Precision-Recall Curve - {model_name}\")\n",
    "        plt.xlabel(\"Recall\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ed48cf-dda9-4e49-ab2e-e2a633301207",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we performed a comparative study of several machine learning models for the detection of cardiomegaly based on extracted features. The models included **KNN, Decision Tree, SVM, Logistic Regression, and Random Forest**. \n",
    "\n",
    "We applied preprocessing with `StandardScaler`, hyperparameter tuning with `GridSearchCV`, and robust evaluation using **cross-validation** with metrics such as **Accuracy, F1-score, and ROC AUC**. Additionally, we visualized the results through **ROC/PR curves** and **confusion matrices** to better understand the performance of each model.\n",
    "\n",
    "From the experiments, we observed that different models capture different aspects of the data, and ensemble methods such as Random Forest tend to provide more stable results. However, even simpler models like Logistic Regression or KNN, when tuned properly, achieved competitive results.  \n",
    "\n",
    "This work demonstrates a full end-to-end machine learning workflow:  \n",
    "- **data preprocessing**,  \n",
    "- **model training**,  \n",
    "- **hyperparameter optimization**,  \n",
    "- **evaluation**,  \n",
    "- and **result visualization**.  \n",
    "\n",
    "These experiments form a solid foundation for future improvements, such as testing more advanced algorithms (e.g., gradient boosting methods) or applying feature engineering techniques.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
