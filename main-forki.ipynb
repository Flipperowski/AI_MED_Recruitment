{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caabd541",
   "metadata": {},
   "source": [
    "# Cardiomegaly Classification – Model Comparison\n",
    "\n",
    "This notebook explores different machine learning models for predicting **cardiomegaly** from radiographic-derived features. \n",
    "\n",
    "Steps:\n",
    "1. Data preprocessing\n",
    "2. Baseline KNN model (with hyperparameter tuning)\n",
    "3. Decision Tree model (manual hyperparameter tuning)\n",
    "4. Evaluation using Accuracy, F1-score, and ROC AUC\n",
    "\n",
    "Dataset size: 50 patients (small sample → high variance expected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c531282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc, f1_score, roc_auc_score\n",
    "\n",
    "#Evaluation function\n",
    "def evaluate_model(model, X, y, cv=5):\n",
    "    # Accuracy\n",
    "    acc = cross_val_score(model, X, y, cv=cv, scoring=\"accuracy\")\n",
    "    print(\"\\nAccuracy scores:\", np.round(acc, 2))\n",
    "    print(f\"Mean Accuracy: {np.mean(acc):.3f} | Std: {np.std(acc):.3f}\")\n",
    "\n",
    "    # F1\n",
    "    f1 = cross_val_score(model, X, y, cv=cv, scoring=\"f1\")\n",
    "    print(\"\\nF1 scores:\", np.round(f1, 2))\n",
    "    print(f\"Mean F1: {np.mean(f1):.3f} | Std: {np.std(f1):.3f}\")\n",
    "\n",
    "    # ROC AUC\n",
    "    auc = cross_val_score(model, X, y, cv=cv, scoring=\"roc_auc\")\n",
    "    print(\"\\nROC AUC scores:\", np.round(auc, 2))\n",
    "    print(f\"Mean ROC AUC: {np.mean(auc):.3f} | Std: {np.std(auc):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172a09de",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "- Import dataset (`task_data.csv`)\n",
    "- Convert numeric columns (comma → dot)\n",
    "- Define features (X) and target (y)\n",
    "- Train-test split (80/20)\n",
    "- Apply `StandardScaler` for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6868af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the data from CSV (80% training, 20% testing)\n",
    "data = pd.read_csv(\"task_data.csv\")\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "numeric_cols = [\n",
    "    \"Heart width\", \"Lung width\", \"CTR - Cardiothoracic Ratio\", \"xx\", \"yy\", \"xy\", \"normalized_diff\",\n",
    "    \"Inscribed circle radius\", \"Polygon Area Ratio\", \"Heart perimeter\", \"Heart area\", \"Lung area\"\n",
    "]\n",
    "\n",
    "#Repairing data types\n",
    "for col in numeric_cols:\n",
    "    data[col] = data[col].astype(str).str.replace(\",\", \".\", regex=True).astype(float)\n",
    "    \n",
    "X = data[[\n",
    "    \"Heart width\", \"Lung width\", \"CTR - Cardiothoracic Ratio\",\n",
    "    \"xx\", \"yy\", \"xy\", \"normalized_diff\",\n",
    "    \"Inscribed circle radius\", \"Polygon Area Ratio\",\n",
    "    \"Heart perimeter\", \"Heart area\", \"Lung area\"\n",
    "]]\n",
    "\n",
    "#Selecting targeted column (Cardiomegaly)\n",
    "y = data[\"Cardiomegaly\"]\n",
    "\n",
    "#Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Creating a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#Fitting and applying scaler\n",
    "X_scaled_train = scaler.fit_transform(X_train)\n",
    "X_scaled_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb6b797",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)\n",
    "- Pipeline with scaling\n",
    "- GridSearchCV with repeated stratified k-fold\n",
    "- Hyperparameters: neighbors, weights, metric, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befdd14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining hyperparameters\n",
    "param_grid = {\n",
    "    \"model__n_neighbors\": [1, 3, 5, 7, 9, 11, 13, 15, 17, 19],\n",
    "    \"model__weights\": [\"uniform\", \"distance\"],\n",
    "    \"model__metric\": [\"minkowski\", \"manhattan\", \"euclidean\"],\n",
    "    \"model__p\": [1, 2]\n",
    "}\n",
    "\n",
    "#Setting up the cross-validation strategy\n",
    "rskf = RepeatedStratifiedKFold(\n",
    "    n_splits=5,\n",
    "    n_repeats=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#Applying K-Nearest Neighbors (KNN) Classifier\n",
    "pipe_knn = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "#Initializing the Grid Search for the KNN model\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipe_knn,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=rskf,\n",
    "    verbose=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#Training\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_knn = grid_search.best_estimator_\n",
    "\n",
    "#Displaying results\n",
    "print(\"General test --- KNN Model \\n\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best accuracy (averaged CV): {grid_search.best_score_:.4f}\\n\\n\")\n",
    "\n",
    "evaluate_model(best_knn, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1dcad3",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "- Custom tuned hyperparameters (max depth, entropy, min samples)\n",
    "- Compared with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9d08c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree\n",
    "clf_tree = DecisionTreeClassifier(\n",
    "    max_depth=5,\n",
    "    criterion='entropy',\n",
    "    min_samples_split=8,\n",
    "    min_samples_leaf=8,\n",
    "    class_weight=None,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#Training\n",
    "clf_tree.fit(X_train, y_train)\n",
    "\n",
    "cv_score = np.round(cross_val_score(clf_tree, X_train, y_train),2 )\n",
    "\n",
    "#Displaying results\n",
    "print(\"\\n\\nGeneral test --- Decision Tree \\n\")\n",
    "\n",
    "print(f\"Scores of training data cross-validation (each fold):\")\n",
    "list(map(print, cv_score))\n",
    "print(f\"\\nCross-validation mean score: {np.mean(cv_score):.3}\")\n",
    "print(f\"Standard deviation of CV score: {np.std(cv_score):.3f}\\n\\n\")\n",
    "\n",
    "evaluate_model(clf_tree, X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
